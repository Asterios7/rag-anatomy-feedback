{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a11851",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53bc908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./app')\n",
    "from pydantic import BaseModel, Field \n",
    "from llm import async_response_openai, async_embed_text, GenText\n",
    "from ranker import retrieve_top_k\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca8780",
   "metadata": {},
   "source": [
    "## Definitions Eval Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionMultipleChoice(BaseModel):\n",
    "    question: str = Field(description='The question')\n",
    "    answers: List[str] = Field(description='The multiple choice answers')\n",
    "    correct_answer: str = Field(description='The correct answer')\n",
    "\n",
    "class QuestionMultipleChoiceList(BaseModel):\n",
    "    question_list: List[QuestionMultipleChoice]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_dir = \"book_eval\"   # top-level book directory\n",
    "\n",
    "files = os.listdir(root_dir)\n",
    "text_list = []\n",
    "for file in files:\n",
    "\n",
    "        filepath = os.path.join(root_dir, file)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            # print(f\"\\n--- {file} ---\")\n",
    "            text = f.read()\n",
    "        text_list.append(text.split('====='))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e10af",
   "metadata": {},
   "source": [
    "## Extract multiple choice questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98fe2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompts:\n",
    "    @classmethod\n",
    "    def multiple_choice(cls, text: str):\n",
    "        prompt=f\"\"\" \n",
    "            You help me organize the multiple questions from a text into a json\n",
    "            with fields: question, answers, correct_answer.\n",
    "            Always keep the letter of the answer along with the answer text.\n",
    "            Always keep the original, provided text unchanged.\n",
    "            This are the questions and answers:\n",
    "            {text}\n",
    "            \"\"\"\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e738c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process results\n",
    "results = []\n",
    "for i, text in enumerate(text_list, start=1):\n",
    "    # print(i)\n",
    "    # print(text[0])\n",
    "    result = await async_response_openai(\n",
    "        user_prompt=Prompts.multiple_choice(text=text[0]),\n",
    "        model='gpt-4.1',\n",
    "        response_model=QuestionMultipleChoiceList\n",
    "        )\n",
    "    \n",
    "    for model in result.question_list:\n",
    "        result_dict: dict = model.__dict__\n",
    "        result_dict.update(chapter = i)\n",
    "        result_dict.update(answers = \";\".join(result_dict['answers']))\n",
    "        results.append(result_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_parquet(\"book_eval_mc.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3786e56",
   "metadata": {},
   "source": [
    "## Evaluation RAG (Multiple Choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d24184",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_TOP_K = 1\n",
    "LLM = 'gpt-5.1'\n",
    "RAG_PARTITION = \"book_partition_full\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Load rag partition\n",
    "df_rag = pd.read_parquet(f'app/db/{RAG_PARTITION}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89986fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random answers for evaluation\n",
    "df = pd.read_parquet(\"book_eval_mc.parquet\")\n",
    "df_eval = df\n",
    "df_eval = df_eval.assign(\n",
    "    student_answer = None, \n",
    "    is_correct_llm = None, \n",
    "    citations = None, \n",
    "    top_k = RETRIEVAL_TOP_K,\n",
    "    llm = LLM,\n",
    "    random_seed = RANDOM_SEED,\n",
    "    rag_partition=RAG_PARTITION\n",
    "    ) \n",
    "eval_dicts = df_eval.to_dict(orient='records')\n",
    "random.seed(RANDOM_SEED)  # fixed seed â†’ reproducible result\n",
    "\n",
    "random_answers_indexes = [random.randint(0, 3) for _ in range(len(eval_dicts))]\n",
    "\n",
    "eval_dicts_with_dummy_answers = []\n",
    "for index, eval_dict in zip(random_answers_indexes, eval_dicts):\n",
    "    d = eval_dict.copy()\n",
    "    answers = d['answers'].split(\";\")\n",
    "    d.update(student_answer = answers[index])\n",
    "    eval_dicts_with_dummy_answers.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class Prompts:\n",
    "    @classmethod\n",
    "    async def feedback(\n",
    "        cls,\n",
    "        question: str,\n",
    "        answers: str,\n",
    "        student_answer: str,\n",
    "        retrieved_text: str,\n",
    "    ) -> Tuple[str, str]:\n",
    "\n",
    "        system_prompt = \"\"\"\n",
    "        You are an expert anatomy tutor providing feedback to a medical student.\n",
    "        Your feedback must be grounded ONLY in the retrieved anatomy text provided.\n",
    "        \n",
    "        RULES:\n",
    "        - Do NOT add information not present in the retrieved text.\n",
    "        - If the retrieved text does not contain the answer, say so.\n",
    "        - Provide feedback that is accurate, concise, and educational.\n",
    "        - Highlight what is correct, what is incorrect, and provide the correct info (only if found in the text).\n",
    "        - Use a supportive and encouraging tone.\n",
    "        - Do NOT mention the rules to the student.\n",
    "        \"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "        Student Question:\n",
    "        {question}\n",
    "\n",
    "        Multiple Choice Answers:\n",
    "        {answers}\n",
    "\n",
    "        Student Answer:\n",
    "        {student_answer}\n",
    "\n",
    "        Retrieved Text (source of truth):\n",
    "        {retrieved_text}\n",
    "\n",
    "        Using ONLY the retrieved text, judge whether the student answer is correct\n",
    "\n",
    "        **Result**\n",
    "        Return \"yes\" if it is correct, \"no\" if it is not correct \n",
    "\n",
    "        If the retrieved text does not include enough information to evaluate the student's answer, respond with:\n",
    "        \"missing info\"\n",
    "        \"\"\"\n",
    "\n",
    "        return system_prompt, user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dicts_judge = []\n",
    "for eval_dict in eval_dicts_with_dummy_answers:\n",
    "    d: dict = eval_dict.copy()\n",
    "    question = d['question'].split('. ')[-1]\n",
    "    \n",
    "    # Embed question\n",
    "    question_text_embedding = await async_embed_text(text=question)\n",
    "\n",
    "    # Retrieve text and citations\n",
    "    df_rag_ranked = retrieve_top_k(\n",
    "        df_rag=df_rag,\n",
    "        query_embedding=question_text_embedding,\n",
    "        top_k=RETRIEVAL_TOP_K\n",
    "    )\n",
    "    retrieved_text = \" \\n\".join(df_rag_ranked['subchapter_text'].values)\n",
    "\n",
    "    cols = ['chapter_title', 'subchapter_number', 'subchapter_title', 'subchapter_page']\n",
    "    citations = \";\".join(\n",
    "        df_rag_ranked[cols]\n",
    "        .astype(str)\n",
    "        .agg(' | '.join, axis=1)\n",
    "        .tolist()\n",
    "        )\n",
    "    d.update(citations=citations)\n",
    "    \n",
    "    # Judge student answer\n",
    "    system_prompt, user_prompt = await Prompts.feedback(\n",
    "        question=question,\n",
    "        student_answer=d['student_answer'],\n",
    "        answers=d['answers'],\n",
    "        retrieved_text=retrieved_text\n",
    "    )\n",
    "    \n",
    "    result: GenText = await async_response_openai(\n",
    "        user_prompt=user_prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        model=LLM,\n",
    "        response_model=GenText\n",
    "        )\n",
    "    d.update(is_correct_llm = result.text)\n",
    "\n",
    "    # Append results\n",
    "    eval_dicts_judge.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cac8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_final = pd.DataFrame(eval_dicts_judge)\n",
    "df_eval_final.to_parquet(\n",
    "    f\"eval_results/multiple_choice-partition_{RAG_PARTITION}__top_k_{RETRIEVAL_TOP_K}__model_{LLM}.parquet\",\n",
    "    index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b431c5d7",
   "metadata": {},
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79298a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_final = pd.read_parquet(\n",
    "    \"eval_results/multiple_choice-partition_book_partition_full__top_k_1__model_gpt-5.1.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ec419a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'answers', 'correct_answer', 'chapter', 'student_answer',\n",
       "       'is_correct_llm', 'citations', 'top_k', 'llm', 'random_seed',\n",
       "       'rag_partition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a29fbb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = df_eval_final[['correct_answer', 'student_answer', 'is_correct_llm']]\n",
    "df_acc = df_acc.assign(is_correct=None, match = None)\n",
    "df_acc['is_correct'] = df_acc['correct_answer'].eq(df_acc['student_answer']).map({True: 'yes', False: 'no'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb1484cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether llm judge and true answers agree\n",
    "aggreement = []\n",
    "mismatch = []\n",
    "mismatch_no_info = []\n",
    "for i, row in df_acc.iterrows():\n",
    "    if (row.is_correct_llm == 'no' and row.is_correct == 'no') or \\\n",
    "        (row.is_correct_llm == 'yes' and row.is_correct == 'yes'): \n",
    "        aggreement.append(i)\n",
    "    if (row.is_correct_llm == 'yes' and row.is_correct == 'no') or \\\n",
    "        (row.is_correct_llm == 'no' and row.is_correct == 'yes'):\n",
    "        mismatch.append(i)\n",
    "    if (row.is_correct_llm == 'missing info' and row.is_correct == 'no') or \\\n",
    "        (row.is_correct_llm == 'missing info' and row.is_correct == 'yes'):\n",
    "        mismatch_no_info.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f232d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurate: 80.36%\n",
      "Errors: 1.43%\n",
      "Errors retrieval (inconclusive): 18.21%\n"
     ]
    }
   ],
   "source": [
    "# Print percentages\n",
    "accuracy_pct = round((len(aggreement) / len(df_acc)) * 100, 2)\n",
    "errors_pct = round(len(mismatch) / len(df_acc) * 100, 2)\n",
    "errors_retrieval_ptc = round(len(mismatch_no_info) / len(df_acc) * 100, 2)\n",
    "\n",
    "print(f\"Accurate: {accuracy_pct}%\")\n",
    "print(f\"Errors: {errors_pct}%\")\n",
    "print(f\"Errors retrieval (inconclusive): {errors_retrieval_ptc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e169c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
